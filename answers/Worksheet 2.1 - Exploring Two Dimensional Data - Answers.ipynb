{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "# Worksheet 2.1:  Exploring Two Dimensional Data -  Answers\n",
    "This worksheet covers concepts covered in Module 2 - Exploratory Data Analysis in Two Dimensions.  It should take no more than 20-30 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "There are many ways to accomplish the tasks that you are presented with, however you will find that by using the techniques covered in class, the exercises should be relatively simple. \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (http://matplotlib.org/api/pyplot_api.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:32:56.467826Z",
     "start_time": "2020-07-29T17:32:56.458579Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "plt.style.use('ggplot')\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Reading various forms of JSON Data\n",
    "In the `/data/` folder, you will find a series of `.json` files called `dataN.json`, numbered 1-4.  Each file contains the following data:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>birthday</th>\n",
    "        <th>first_name</th>\n",
    "        <th>last_name</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>5\\/3\\/67</td>\n",
    "        <td>Robert</td>\n",
    "        <td>Hernandez</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>8\\/4\\/84</td>\n",
    "        <td>Steve</td>\n",
    "        <td>Smith</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>9\\/13\\/91</td>\n",
    "        <td>Anne</td>\n",
    "        <td>Raps</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>4\\/15\\/75</td>\n",
    "        <td>Alice</td>\n",
    "        <td>Muller</td>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n",
    "Using the `.read_json()` function and the various configuration options, read all these files into a dataframe.  The documentation is available here: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:32:57.924666Z",
     "start_time": "2020-07-29T17:32:57.911441Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_HOME = '../data/'\n",
    "df1 = pd.read_json(DATA_HOME + 'data1.json')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:32:58.536612Z",
     "start_time": "2020-07-29T17:32:58.525933Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_json(DATA_HOME + 'data2.json', orient='index')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:32:59.227531Z",
     "start_time": "2020-07-29T17:32:59.214422Z"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_json(DATA_HOME + 'data3.json', orient='columns')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:32:59.814573Z",
     "start_time": "2020-07-29T17:32:59.798548Z"
    }
   },
   "outputs": [],
   "source": [
    "df4 = pd.read_json(DATA_HOME + 'data4.json', orient='split')\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "In the data file, there is a webserver file called `hackers-access.httpd`.  For this exercise, you will use this file to answer the following questions:\n",
    "1.  Which browsers are the top 10 most used browsers in this data?\n",
    "2.  Which are the top 10 most used operating systems?\n",
    "\n",
    "In order to accomplish this task, do the following:\n",
    "1.  Write a function which takes a User Agent string as an argument and returns the relevant data.  HINT:  You might want to use python's `user_agents` module, the documentation for which is available here: (https://pypi.python.org/pypi/user-agents)\n",
    "2.  Next, apply this function to the column which contains the user agent string.\n",
    "3.  Store this series as a new column in the dataframe\n",
    "4.  Count the occurances of each value in the new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:01.117448Z",
     "start_time": "2020-07-29T17:33:00.788264Z"
    }
   },
   "outputs": [],
   "source": [
    "import apache_log_parser\n",
    "from user_agents import parse\n",
    "#Read in the log file\n",
    "line_parser = apache_log_parser.make_parser(\"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\"\")\n",
    "\n",
    "server_log = open(DATA_HOME + \"hackers-access.httpd\", \"r\")\n",
    "parsed_server_data = []\n",
    "for line in server_log:\n",
    "    data = {}\n",
    "    data = line_parser(line)\n",
    "    parsed_server_data.append( data )\n",
    "\n",
    "server_df = pd.DataFrame( parsed_server_data  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:01.415408Z",
     "start_time": "2020-07-29T17:33:01.386009Z"
    }
   },
   "outputs": [],
   "source": [
    "server_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:01.972694Z",
     "start_time": "2020-07-29T17:33:01.969304Z"
    }
   },
   "outputs": [],
   "source": [
    "#Write the functions\n",
    "def get_os(x):\n",
    "    user_agent = parse(x)\n",
    "    return user_agent.os.family\n",
    "\n",
    "def get_browser(x):\n",
    "    user_agent = parse(x)\n",
    "    return user_agent.browser.family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:02.678234Z",
     "start_time": "2020-07-29T17:33:02.520514Z"
    }
   },
   "outputs": [],
   "source": [
    "#Apply the functions to the dataframe\n",
    "server_df['os'] = server_df['request_header_user_agent'].apply( get_os )\n",
    "server_df['browser'] = server_df['request_header_user_agent'].apply( get_browser )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:03.011677Z",
     "start_time": "2020-07-29T17:33:03.004552Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get the top 10 values\n",
    "server_df['os'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:03.519465Z",
     "start_time": "2020-07-29T17:33:03.511253Z"
    }
   },
   "outputs": [],
   "source": [
    "server_df['browser'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "Using the `dailybots.csv` film, read the file into a DataFrame and perform the following operations:\n",
    "1.  Filter the DataFrame to include bots from the Government/Politics Industry.\n",
    "2.  Calculate the ratio of hosts to orgs and add this as a column to the DataFrame and output the result\n",
    "3.  Calculate the total number of hosts infected by each BotFam in the Government/Politics Industry.  You should use the `groupby()` function which is documented here: (http://pandas.pydata.org/pandas-docs/stable/groupby.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:04.566551Z",
     "start_time": "2020-07-29T17:33:04.553613Z"
    }
   },
   "outputs": [],
   "source": [
    "bots = pd.read_csv(DATA_HOME + 'dailybots.csv')\n",
    "bots.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:05.160685Z",
     "start_time": "2020-07-29T17:33:05.154936Z"
    }
   },
   "outputs": [],
   "source": [
    "gov_bots = bots[['botfam', 'hosts']][bots['industry'] == \"Government/Politics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:05.686749Z",
     "start_time": "2020-07-29T17:33:05.682122Z"
    }
   },
   "outputs": [],
   "source": [
    "gov_bots['ratio'] = bots['hosts'] / bots['orgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:06.200047Z",
     "start_time": "2020-07-29T17:33:06.189289Z"
    }
   },
   "outputs": [],
   "source": [
    "gov_bots.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:06.814857Z",
     "start_time": "2020-07-29T17:33:06.797999Z"
    }
   },
   "outputs": [],
   "source": [
    "gov_bots.groupby('botfam', as_index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4:\n",
    "\n",
    "Read a more ```evil``` JSON ```eve_small.json```, where each line contains a nested JSON object. Derive one DataFrame, where all levels for the ```stats``` key are expanded to a top level column of that DataFrame. Easiest is to natively open the file in Python, loop over each line, use [json.loads](https://docs.python.org/3.5/library/json.html) from the json library, and then [json_normalize](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html) to expand the nested structure to top-level columns, append to a simple Python list and finally call [pd.concat](http://pandas.pydata.org/pandas-docs/version/0.20/generated/pandas.concat.html) on the list to get one complete DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:07.820626Z",
     "start_time": "2020-07-29T17:33:07.815531Z"
    }
   },
   "outputs": [],
   "source": [
    "def nested_json_to_df(fname_str):\n",
    "    with open(fname_str, 'r') as f:\n",
    "        l = []\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            else:\n",
    "                d = json.loads(line)\n",
    "                if 'stats' in d.keys():\n",
    "                    df_tmp = pd.json_normalize(d)\n",
    "                    l.append(df_tmp)       \n",
    "    return pd.concat(l, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:09.051952Z",
     "start_time": "2020-07-29T17:33:08.258895Z"
    }
   },
   "outputs": [],
   "source": [
    "eve = nested_json_to_df(DATA_HOME + 'eve_small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:09.193601Z",
     "start_time": "2020-07-29T17:33:09.174439Z"
    }
   },
   "outputs": [],
   "source": [
    "eve.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "In this exercise, you will learn how to convert a PCAP file into JSON and do some basic summarization of the data.  In the `data` directory, you will find a file called `http.pcap`.  Our first step is to convert this to JSON.  To do this we have installed a python module called `pcapview` (docs available here: https://pydigger.com/pypi/pcapview) which can convert the pcap file to JSON.  \n",
    "\n",
    "Once you've done that, your assignment is to answer the following questions:\n",
    "1.  What are the most frequent source IP addresses?\n",
    "2.  How many differnet source ports were accessed?\n",
    "\n",
    "To do this you will have to load this data into a DataFrame.  Using what we've learned in class, do the following:\n",
    "1.  Load the data into a DataFrame using the technique of your choice\n",
    "2.  Extract the requisite columns from the DataFrame, in this case, you want the source IP and source ports\n",
    "3.  Execute a `value_counts()` on those columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:20.100620Z",
     "start_time": "2020-07-29T17:33:20.071195Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "with open(DATA_HOME + 'http-pcap.json') as data_file:    \n",
    "    pcap_data = json.load(data_file)\n",
    "\n",
    "#Normalize it and load it into a DataFrame\n",
    "df = pd.DataFrame( pd.json_normalize(pcap_data) )\n",
    "\n",
    "#View the results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:21.028646Z",
     "start_time": "2020-07-29T17:33:21.016882Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_json(DATA_HOME + 'http-pcap.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:22.055646Z",
     "start_time": "2020-07-29T17:33:22.051261Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extract the source port and count the unique values\n",
    "df['TCP.sport'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T17:33:22.667440Z",
     "start_time": "2020-07-29T17:33:22.661340Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extract the source IP and count the unique values\n",
    "df['IP.src'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
